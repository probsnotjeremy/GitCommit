{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"sentence-transformers/natural-questions\")\n",
    "\n",
    "# Pick the train split\n",
    "dataset_split = ds[\"train\"]\n",
    "\n",
    "# Convert split into list of dicts\n",
    "rows = dataset_split.to_list()\n",
    "\n",
    "# Each row has {\"query\": ..., \"answer\": ...}\n",
    "dataset = [[row[\"query\"], row[\"answer\"]] for row in rows]\n",
    "\n",
    "# Sample 20 rows\n",
    "subset = random.sample(dataset, min(20, len(dataset)))\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "history = []\n",
    "\n",
    "def ask_phi2(user_input, max_new_tokens=256):\n",
    "    history.append(f\"User: {user_input}\")\n",
    "    prompt = \"\\n\".join(history) + \"\\nAssistant:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Split at the last \"Assistant:\" to get just the assistantâ€™s reply\n",
    "    response = full_output.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "    history.append(f\"Assistant: {response}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def clean_words(text):\n",
    "    \"\"\"Split into lowercase words, remove stopwords.\"\"\"\n",
    "    return [w.lower() for w in text.split() if w.lower() not in stopwords]\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    \"\"\"Return a set of words + their synonyms from WordNet.\"\"\"\n",
    "    expanded = set(words)\n",
    "    for w in words:\n",
    "        for syn in wordnet.synsets(w):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded.add(lemma.name().lower().replace(\"_\", \" \"))\n",
    "    return expanded\n",
    "\n",
    "def similarity(answer, response, use_synonyms=True):\n",
    "    \"\"\"Compute % overlap between answer words and response words.\"\"\"\n",
    "    a_words = clean_words(answer)\n",
    "    r_words = clean_words(response)\n",
    "\n",
    "    if use_synonyms:\n",
    "        a_set = expand_with_synonyms(a_words)\n",
    "        r_set = expand_with_synonyms(r_words)\n",
    "    else:\n",
    "        a_set, r_set = set(a_words), set(r_words)\n",
    "\n",
    "    if not a_set:\n",
    "        return 0\n",
    "    return len(a_set & r_set) / len(a_set) * 100\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "def send_to_llm(question):\n",
    "    global history\n",
    "    history = []\n",
    "    return ask_phi2(question, max_new_tokens=256)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "results = []\n",
    "for i, (q, a) in enumerate(subset):\n",
    "    response = send_to_llm(q)\n",
    "    score = similarity(a, response, use_synonyms=True)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": i,\n",
    "        \"question\": q,\n",
    "        \"expected_answer\": a,\n",
    "        \"llm_response\": response,\n",
    "        \"similarity_percent\": score\n",
    "    })\n",
    "\n",
    "# ----------------------------\n",
    "# Save results without overwriting\n",
    "filename = \"TestResults.json\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        all_results = json.load(f)\n",
    "else:\n",
    "    all_results = []\n",
    "\n",
    "all_results.append(results)\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"Saved\", len(results), \"results to\", filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
