{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.010502105672187274,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00010502105672187273,
      "grad_norm": 4.869820594787598,
      "learning_rate": 0.00019998109620979006,
      "loss": 8.7767,
      "step": 10
    },
    {
      "epoch": 0.00021004211344374547,
      "grad_norm": 13.937301635742188,
      "learning_rate": 0.0001999600919984457,
      "loss": 5.4977,
      "step": 20
    },
    {
      "epoch": 0.0003150631701656182,
      "grad_norm": 13.367316246032715,
      "learning_rate": 0.00019993908778710134,
      "loss": 4.279,
      "step": 30
    },
    {
      "epoch": 0.00042008422688749093,
      "grad_norm": 10.721000671386719,
      "learning_rate": 0.00019991808357575693,
      "loss": 4.6061,
      "step": 40
    },
    {
      "epoch": 0.0005251052836093636,
      "grad_norm": 5.159592151641846,
      "learning_rate": 0.00019989707936441257,
      "loss": 5.0406,
      "step": 50
    },
    {
      "epoch": 0.0006301263403312364,
      "grad_norm": 4.853556156158447,
      "learning_rate": 0.0001998760751530682,
      "loss": 4.5537,
      "step": 60
    },
    {
      "epoch": 0.0007351473970531092,
      "grad_norm": 4.543334007263184,
      "learning_rate": 0.00019985507094172383,
      "loss": 4.3593,
      "step": 70
    },
    {
      "epoch": 0.0008401684537749819,
      "grad_norm": 4.744979381561279,
      "learning_rate": 0.00019983406673037947,
      "loss": 4.9324,
      "step": 80
    },
    {
      "epoch": 0.0009451895104968546,
      "grad_norm": 2.920069456100464,
      "learning_rate": 0.00019981306251903505,
      "loss": 4.6078,
      "step": 90
    },
    {
      "epoch": 0.0010502105672187273,
      "grad_norm": 20.802263259887695,
      "learning_rate": 0.0001997920583076907,
      "loss": 4.8027,
      "step": 100
    },
    {
      "epoch": 0.0011552316239406002,
      "grad_norm": 13.444307327270508,
      "learning_rate": 0.00019977105409634633,
      "loss": 3.6777,
      "step": 110
    },
    {
      "epoch": 0.001260252680662473,
      "grad_norm": 6.423947811126709,
      "learning_rate": 0.00019975004988500195,
      "loss": 5.0883,
      "step": 120
    },
    {
      "epoch": 0.0013652737373843456,
      "grad_norm": 3.366412401199341,
      "learning_rate": 0.0001997290456736576,
      "loss": 4.1983,
      "step": 130
    },
    {
      "epoch": 0.0014702947941062183,
      "grad_norm": 12.414886474609375,
      "learning_rate": 0.0001997080414623132,
      "loss": 4.2425,
      "step": 140
    },
    {
      "epoch": 0.001575315850828091,
      "grad_norm": 14.856581687927246,
      "learning_rate": 0.00019968703725096882,
      "loss": 6.1425,
      "step": 150
    },
    {
      "epoch": 0.0016803369075499637,
      "grad_norm": 12.9804048538208,
      "learning_rate": 0.00019966603303962446,
      "loss": 6.1379,
      "step": 160
    },
    {
      "epoch": 0.0017853579642718364,
      "grad_norm": 19.146648406982422,
      "learning_rate": 0.0001996450288282801,
      "loss": 5.3226,
      "step": 170
    },
    {
      "epoch": 0.0018903790209937091,
      "grad_norm": 6.955646514892578,
      "learning_rate": 0.0001996240246169357,
      "loss": 3.6307,
      "step": 180
    },
    {
      "epoch": 0.001995400077715582,
      "grad_norm": 8.985008239746094,
      "learning_rate": 0.00019960302040559133,
      "loss": 4.1359,
      "step": 190
    },
    {
      "epoch": 0.0021004211344374545,
      "grad_norm": 6.198591709136963,
      "learning_rate": 0.00019958201619424694,
      "loss": 3.8264,
      "step": 200
    },
    {
      "epoch": 0.0022054421911593272,
      "grad_norm": 8.930381774902344,
      "learning_rate": 0.00019956101198290258,
      "loss": 3.6928,
      "step": 210
    },
    {
      "epoch": 0.0023104632478812004,
      "grad_norm": 6.631084442138672,
      "learning_rate": 0.00019954000777155822,
      "loss": 4.0457,
      "step": 220
    },
    {
      "epoch": 0.002415484304603073,
      "grad_norm": 5.600887775421143,
      "learning_rate": 0.00019951900356021384,
      "loss": 3.6047,
      "step": 230
    },
    {
      "epoch": 0.002520505361324946,
      "grad_norm": 5.921529293060303,
      "learning_rate": 0.00019949799934886945,
      "loss": 3.8305,
      "step": 240
    },
    {
      "epoch": 0.0026255264180468185,
      "grad_norm": 10.006867408752441,
      "learning_rate": 0.0001994769951375251,
      "loss": 4.5627,
      "step": 250
    },
    {
      "epoch": 0.002730547474768691,
      "grad_norm": 2.4560508728027344,
      "learning_rate": 0.0001994559909261807,
      "loss": 4.4605,
      "step": 260
    },
    {
      "epoch": 0.002835568531490564,
      "grad_norm": 4.200296878814697,
      "learning_rate": 0.00019943498671483634,
      "loss": 4.2072,
      "step": 270
    },
    {
      "epoch": 0.0029405895882124366,
      "grad_norm": 8.067315101623535,
      "learning_rate": 0.00019941398250349196,
      "loss": 4.5195,
      "step": 280
    },
    {
      "epoch": 0.0030456106449343093,
      "grad_norm": 4.244932174682617,
      "learning_rate": 0.00019939297829214757,
      "loss": 3.2858,
      "step": 290
    },
    {
      "epoch": 0.003150631701656182,
      "grad_norm": 8.104395866394043,
      "learning_rate": 0.0001993719740808032,
      "loss": 4.5903,
      "step": 300
    },
    {
      "epoch": 0.0032556527583780547,
      "grad_norm": 4.230822563171387,
      "learning_rate": 0.00019935096986945883,
      "loss": 3.8658,
      "step": 310
    },
    {
      "epoch": 0.0033606738150999274,
      "grad_norm": 13.62906551361084,
      "learning_rate": 0.00019932996565811447,
      "loss": 5.1101,
      "step": 320
    },
    {
      "epoch": 0.0034656948718218,
      "grad_norm": 2.8755171298980713,
      "learning_rate": 0.00019930896144677008,
      "loss": 3.8224,
      "step": 330
    },
    {
      "epoch": 0.003570715928543673,
      "grad_norm": 4.998781204223633,
      "learning_rate": 0.0001992879572354257,
      "loss": 3.9122,
      "step": 340
    },
    {
      "epoch": 0.0036757369852655456,
      "grad_norm": 3.931281566619873,
      "learning_rate": 0.00019926695302408134,
      "loss": 4.8158,
      "step": 350
    },
    {
      "epoch": 0.0037807580419874183,
      "grad_norm": 2.83809232711792,
      "learning_rate": 0.00019924594881273698,
      "loss": 3.6848,
      "step": 360
    },
    {
      "epoch": 0.0038857790987092914,
      "grad_norm": 8.635156631469727,
      "learning_rate": 0.0001992249446013926,
      "loss": 4.9294,
      "step": 370
    },
    {
      "epoch": 0.003990800155431164,
      "grad_norm": 11.538302421569824,
      "learning_rate": 0.0001992039403900482,
      "loss": 4.394,
      "step": 380
    },
    {
      "epoch": 0.004095821212153037,
      "grad_norm": 6.466172695159912,
      "learning_rate": 0.00019918293617870385,
      "loss": 3.9969,
      "step": 390
    },
    {
      "epoch": 0.004200842268874909,
      "grad_norm": 4.662842273712158,
      "learning_rate": 0.00019916193196735946,
      "loss": 4.3345,
      "step": 400
    },
    {
      "epoch": 0.004305863325596782,
      "grad_norm": 5.257424831390381,
      "learning_rate": 0.0001991409277560151,
      "loss": 5.1923,
      "step": 410
    },
    {
      "epoch": 0.0044108843823186545,
      "grad_norm": 14.386260986328125,
      "learning_rate": 0.00019911992354467071,
      "loss": 3.6243,
      "step": 420
    },
    {
      "epoch": 0.004515905439040528,
      "grad_norm": 5.2265119552612305,
      "learning_rate": 0.00019909891933332633,
      "loss": 4.3022,
      "step": 430
    },
    {
      "epoch": 0.004620926495762401,
      "grad_norm": 13.051470756530762,
      "learning_rate": 0.00019907791512198197,
      "loss": 4.2041,
      "step": 440
    },
    {
      "epoch": 0.004725947552484273,
      "grad_norm": 3.726569890975952,
      "learning_rate": 0.00019905691091063758,
      "loss": 3.1589,
      "step": 450
    },
    {
      "epoch": 0.004830968609206146,
      "grad_norm": 9.096291542053223,
      "learning_rate": 0.00019903590669929322,
      "loss": 3.5599,
      "step": 460
    },
    {
      "epoch": 0.0049359896659280185,
      "grad_norm": 7.432946681976318,
      "learning_rate": 0.00019901490248794886,
      "loss": 4.6484,
      "step": 470
    },
    {
      "epoch": 0.005041010722649892,
      "grad_norm": 9.2013578414917,
      "learning_rate": 0.00019899389827660445,
      "loss": 4.8363,
      "step": 480
    },
    {
      "epoch": 0.005146031779371764,
      "grad_norm": 2.9079489707946777,
      "learning_rate": 0.0001989728940652601,
      "loss": 4.3936,
      "step": 490
    },
    {
      "epoch": 0.005251052836093637,
      "grad_norm": 8.802605628967285,
      "learning_rate": 0.00019895188985391573,
      "loss": 5.1595,
      "step": 500
    },
    {
      "epoch": 0.005251052836093637,
      "eval_loss": 4.808682918548584,
      "eval_runtime": 420.6292,
      "eval_samples_per_second": 1.189,
      "eval_steps_per_second": 1.189,
      "step": 500
    },
    {
      "epoch": 0.005356073892815509,
      "grad_norm": 4.243188381195068,
      "learning_rate": 0.00019893088564257135,
      "loss": 4.3257,
      "step": 510
    },
    {
      "epoch": 0.005461094949537382,
      "grad_norm": 2.710127592086792,
      "learning_rate": 0.000198909881431227,
      "loss": 3.4729,
      "step": 520
    },
    {
      "epoch": 0.005566116006259255,
      "grad_norm": 10.310261726379395,
      "learning_rate": 0.00019888887721988257,
      "loss": 4.0451,
      "step": 530
    },
    {
      "epoch": 0.005671137062981128,
      "grad_norm": 2.4621243476867676,
      "learning_rate": 0.00019886787300853821,
      "loss": 4.2952,
      "step": 540
    },
    {
      "epoch": 0.005776158119703,
      "grad_norm": 10.86684799194336,
      "learning_rate": 0.00019884686879719385,
      "loss": 5.1609,
      "step": 550
    },
    {
      "epoch": 0.005881179176424873,
      "grad_norm": 13.774720191955566,
      "learning_rate": 0.00019882586458584947,
      "loss": 4.1656,
      "step": 560
    },
    {
      "epoch": 0.0059862002331467455,
      "grad_norm": 2.520660400390625,
      "learning_rate": 0.0001988048603745051,
      "loss": 3.357,
      "step": 570
    },
    {
      "epoch": 0.006091221289868619,
      "grad_norm": 2.8599581718444824,
      "learning_rate": 0.00019878385616316072,
      "loss": 4.7603,
      "step": 580
    },
    {
      "epoch": 0.006196242346590492,
      "grad_norm": 17.98204231262207,
      "learning_rate": 0.00019876285195181634,
      "loss": 4.7993,
      "step": 590
    },
    {
      "epoch": 0.006301263403312364,
      "grad_norm": 4.457610607147217,
      "learning_rate": 0.00019874184774047198,
      "loss": 4.1844,
      "step": 600
    },
    {
      "epoch": 0.006406284460034237,
      "grad_norm": 4.027440547943115,
      "learning_rate": 0.00019872084352912762,
      "loss": 4.0912,
      "step": 610
    },
    {
      "epoch": 0.0065113055167561095,
      "grad_norm": 4.35093355178833,
      "learning_rate": 0.00019869983931778323,
      "loss": 5.355,
      "step": 620
    },
    {
      "epoch": 0.006616326573477983,
      "grad_norm": 9.585308074951172,
      "learning_rate": 0.00019867883510643885,
      "loss": 5.7512,
      "step": 630
    },
    {
      "epoch": 0.006721347630199855,
      "grad_norm": 3.1484580039978027,
      "learning_rate": 0.00019865783089509446,
      "loss": 3.7992,
      "step": 640
    },
    {
      "epoch": 0.006826368686921728,
      "grad_norm": 1.471338152885437,
      "learning_rate": 0.0001986368266837501,
      "loss": 3.3313,
      "step": 650
    },
    {
      "epoch": 0.0069313897436436,
      "grad_norm": 3.09118390083313,
      "learning_rate": 0.00019861582247240574,
      "loss": 3.4962,
      "step": 660
    },
    {
      "epoch": 0.0070364108003654734,
      "grad_norm": 10.803742408752441,
      "learning_rate": 0.00019859481826106136,
      "loss": 4.2801,
      "step": 670
    },
    {
      "epoch": 0.007141431857087346,
      "grad_norm": 7.013978958129883,
      "learning_rate": 0.00019857381404971697,
      "loss": 5.05,
      "step": 680
    },
    {
      "epoch": 0.007246452913809219,
      "grad_norm": 15.13983154296875,
      "learning_rate": 0.0001985528098383726,
      "loss": 4.6385,
      "step": 690
    },
    {
      "epoch": 0.007351473970531091,
      "grad_norm": 2.7812612056732178,
      "learning_rate": 0.00019853180562702822,
      "loss": 4.1365,
      "step": 700
    },
    {
      "epoch": 0.007456495027252964,
      "grad_norm": 3.3531460762023926,
      "learning_rate": 0.00019851080141568386,
      "loss": 4.6924,
      "step": 710
    },
    {
      "epoch": 0.0075615160839748365,
      "grad_norm": 4.99115514755249,
      "learning_rate": 0.00019848979720433948,
      "loss": 4.7374,
      "step": 720
    },
    {
      "epoch": 0.00766653714069671,
      "grad_norm": 13.965648651123047,
      "learning_rate": 0.0001984687929929951,
      "loss": 4.7559,
      "step": 730
    },
    {
      "epoch": 0.007771558197418583,
      "grad_norm": 3.9886562824249268,
      "learning_rate": 0.00019844778878165073,
      "loss": 4.2244,
      "step": 740
    },
    {
      "epoch": 0.007876579254140455,
      "grad_norm": 8.563384056091309,
      "learning_rate": 0.00019842678457030635,
      "loss": 4.5482,
      "step": 750
    },
    {
      "epoch": 0.007981600310862327,
      "grad_norm": 9.87619400024414,
      "learning_rate": 0.000198405780358962,
      "loss": 5.5322,
      "step": 760
    },
    {
      "epoch": 0.008086621367584201,
      "grad_norm": 11.258684158325195,
      "learning_rate": 0.0001983847761476176,
      "loss": 3.4965,
      "step": 770
    },
    {
      "epoch": 0.008191642424306074,
      "grad_norm": 8.480742454528809,
      "learning_rate": 0.00019836377193627322,
      "loss": 4.1686,
      "step": 780
    },
    {
      "epoch": 0.008296663481027946,
      "grad_norm": 5.761425971984863,
      "learning_rate": 0.00019834276772492886,
      "loss": 4.1372,
      "step": 790
    },
    {
      "epoch": 0.008401684537749818,
      "grad_norm": 3.258472442626953,
      "learning_rate": 0.0001983217635135845,
      "loss": 4.261,
      "step": 800
    },
    {
      "epoch": 0.008506705594471692,
      "grad_norm": 12.680307388305664,
      "learning_rate": 0.0001983007593022401,
      "loss": 4.8514,
      "step": 810
    },
    {
      "epoch": 0.008611726651193564,
      "grad_norm": 5.024438381195068,
      "learning_rate": 0.00019827975509089572,
      "loss": 4.1526,
      "step": 820
    },
    {
      "epoch": 0.008716747707915437,
      "grad_norm": 3.723180055618286,
      "learning_rate": 0.00019825875087955137,
      "loss": 4.5171,
      "step": 830
    },
    {
      "epoch": 0.008821768764637309,
      "grad_norm": 2.078502655029297,
      "learning_rate": 0.00019823774666820698,
      "loss": 4.0341,
      "step": 840
    },
    {
      "epoch": 0.008926789821359183,
      "grad_norm": 3.5185749530792236,
      "learning_rate": 0.00019821674245686262,
      "loss": 4.5769,
      "step": 850
    },
    {
      "epoch": 0.009031810878081055,
      "grad_norm": 4.770508766174316,
      "learning_rate": 0.00019819573824551823,
      "loss": 3.6531,
      "step": 860
    },
    {
      "epoch": 0.009136831934802928,
      "grad_norm": 7.163434028625488,
      "learning_rate": 0.00019817473403417385,
      "loss": 4.1319,
      "step": 870
    },
    {
      "epoch": 0.009241852991524802,
      "grad_norm": 2.619762420654297,
      "learning_rate": 0.0001981537298228295,
      "loss": 3.4595,
      "step": 880
    },
    {
      "epoch": 0.009346874048246674,
      "grad_norm": 3.7716526985168457,
      "learning_rate": 0.0001981327256114851,
      "loss": 4.4278,
      "step": 890
    },
    {
      "epoch": 0.009451895104968546,
      "grad_norm": 9.093047142028809,
      "learning_rate": 0.00019811172140014074,
      "loss": 4.9507,
      "step": 900
    },
    {
      "epoch": 0.009556916161690418,
      "grad_norm": 2.5745503902435303,
      "learning_rate": 0.00019809071718879638,
      "loss": 4.1238,
      "step": 910
    },
    {
      "epoch": 0.009661937218412292,
      "grad_norm": 3.2580337524414062,
      "learning_rate": 0.00019806971297745197,
      "loss": 5.0004,
      "step": 920
    },
    {
      "epoch": 0.009766958275134165,
      "grad_norm": 3.1869959831237793,
      "learning_rate": 0.0001980487087661076,
      "loss": 4.477,
      "step": 930
    },
    {
      "epoch": 0.009871979331856037,
      "grad_norm": 2.1383728981018066,
      "learning_rate": 0.00019802770455476325,
      "loss": 3.3269,
      "step": 940
    },
    {
      "epoch": 0.00997700038857791,
      "grad_norm": 4.681234836578369,
      "learning_rate": 0.00019800670034341887,
      "loss": 4.6253,
      "step": 950
    },
    {
      "epoch": 0.010082021445299783,
      "grad_norm": 5.453263282775879,
      "learning_rate": 0.0001979856961320745,
      "loss": 4.8064,
      "step": 960
    },
    {
      "epoch": 0.010187042502021655,
      "grad_norm": 3.473754644393921,
      "learning_rate": 0.0001979646919207301,
      "loss": 3.3965,
      "step": 970
    },
    {
      "epoch": 0.010292063558743528,
      "grad_norm": 3.793313503265381,
      "learning_rate": 0.00019794368770938573,
      "loss": 3.9704,
      "step": 980
    },
    {
      "epoch": 0.0103970846154654,
      "grad_norm": 3.2806572914123535,
      "learning_rate": 0.00019792268349804137,
      "loss": 4.2361,
      "step": 990
    },
    {
      "epoch": 0.010502105672187274,
      "grad_norm": 2.0540196895599365,
      "learning_rate": 0.000197901679286697,
      "loss": 4.1639,
      "step": 1000
    },
    {
      "epoch": 0.010502105672187274,
      "eval_loss": 4.447461128234863,
      "eval_runtime": 368.4168,
      "eval_samples_per_second": 1.357,
      "eval_steps_per_second": 1.357,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 95219,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4076320849920000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
